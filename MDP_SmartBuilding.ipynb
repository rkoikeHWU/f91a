{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRn10g7gK/bWSinvU8zyqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkoikeHWU/f91a/blob/main/MDP_SmartBuilding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" code from https://github.com/aimacode/aima-python/blob/master/mdp4e.py adapted to SmartBuilding. \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Markov Decision Processes (Chapter 16)\n",
        "\n",
        "First we define an MDP, and the special case of a GridMDP, in which\n",
        "states are laid out in a 2-dimensional grid. We also represent a policy\n",
        "as a dictionary of {state: action} pairs, and a Utility function as a\n",
        "dictionary of {state: number} pairs. We then define the value_iteration\n",
        "and policy_iteration algorithms.\n",
        "\"\"\"\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# Core MDP class (adapted from AIMA)\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
        "    and reward function. We also keep track of a gamma value, for use by\n",
        "    algorithms. The transition model is represented somewhat differently from\n",
        "    the text. Instead of P(s' | s, a) being a probability number for each\n",
        "    state/state/action triplet, we instead have T(s, a) return a\n",
        "    list of (p, s') pairs. We also keep track of the possible states,\n",
        "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
        "\n",
        "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
        "        if not (0 < gamma <= 1):\n",
        "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
        "\n",
        "        # collect states from transitions table if not passed.\n",
        "        self.states = states or self.get_states_from_transitions(transitions)\n",
        "\n",
        "        self.init = init\n",
        "\n",
        "        if isinstance(actlist, list):\n",
        "            # if actlist is a list, all states have the same actions\n",
        "            self.actlist = actlist\n",
        "\n",
        "        elif isinstance(actlist, dict):\n",
        "            # if actlist is a dict, different actions for each state\n",
        "            self.actlist = actlist\n",
        "\n",
        "        self.terminals = terminals\n",
        "        self.transitions = transitions or {}\n",
        "        if not self.transitions:\n",
        "            print(\"Warning: Transition table is empty.\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.reward = reward or {s: 0 for s in self.states}\n",
        "\n",
        "    def R(self, state):\n",
        "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
        "\n",
        "        return self.reward[state]\n",
        "\n",
        "    def R(self, state, action):\n",
        "        \"\"\"Polymorphic method to retun reward using state/action pair\"\"\"\n",
        "        if action is None:\n",
        "            return 0.0 # no action gets 0 reward\n",
        "\n",
        "        return float(self.reward_sa[(state, action)])\n",
        "\n",
        "    def T(self, state, action):\n",
        "        \"\"\"Transition model. From a state and an action, return a list\n",
        "        of (probability, result-state) pairs.\"\"\"\n",
        "\n",
        "        if not self.transitions:\n",
        "            raise ValueError(\"Transition model is missing\")\n",
        "        else:\n",
        "            return self.transitions[state][action]\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"Return a list of actions that can be performed in this state. By default, a fixed list of actions, except for terminal states. Override this method if you need to specialize by state.\"\"\"\n",
        "\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.actlist\n",
        "\n",
        "    def get_states_from_transitions(self, transitions):\n",
        "        if isinstance(transitions, dict):\n",
        "            s1 = set(transitions.keys())\n",
        "            s2 = set(tr[1] for actions in transitions.values()\n",
        "                     for effects in actions.values()\n",
        "                     for tr in effects)\n",
        "            return s1.union(s2)\n",
        "        else:\n",
        "            print('Could not retrieve states from transitions')\n",
        "            return None\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# 16.1.3 The Bellman equation for utilities\n",
        "\n",
        "def q_value(mdp, s, a, U):\n",
        "    \"\"\"Reward depends on state, action) not just state. So we use mdp.R(s,a) to get q value.\"\"\"\n",
        "\n",
        "    if not a:\n",
        "        return 0.0 # if no action return 0\n",
        "\n",
        "    res = 0.0\n",
        "    for p, s_prime in mdp.T(s, a):\n",
        "        res += p * (mdp.R(s, a) + mdp.gamma * U[s_prime])\n",
        "    return res\n",
        "#______________________________________________________________________________\n",
        "# Value Iteration (prints V0, V1, V2, V3 then converges)\n",
        "\n",
        "def value_iteration(mdp, epsilon=0.001):\n",
        "\n",
        "    U1 = {s: 0.0 for s in mdp.states}\n",
        "    gamma = mdp.gamma\n",
        "\n",
        "    def show(k, U):\n",
        "   #     print(f\"V{k}: \" + \", \".join(f\"{s}={U[s]:.4f}\" for s in order))\n",
        "      print(f\"V{k}: {U['L']}, {U['M']}, {U['H']}\")\n",
        "      print(\"-------------------------\")\n",
        "\n",
        "    # Print V0\n",
        "    show(0, U1)\n",
        "\n",
        "    k = 0\n",
        "    while True:\n",
        "        k += 1\n",
        "        U = U1.copy()\n",
        "        delta = 0.0\n",
        "\n",
        "        for s in mdp.states:\n",
        "            U1[s] = max(q_value(mdp, s, a, U) for a in mdp.actions(s))\n",
        "            delta = max(delta, abs(U1[s] - U[s]))\n",
        "\n",
        "        if k <= 3: # print first 3 only\n",
        "          show(k, U1)\n",
        "\n",
        "        if delta <= epsilon * (1 - gamma) / gamma:\n",
        "            return U1\n",
        "\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# Extract optimal policy\n",
        "\n",
        "def best_policy(mdp, U):\n",
        "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
        "    as a mapping from state to action. [Equation 17.4]\"\"\"\n",
        "\n",
        "    pi = {}\n",
        "    for s in mdp.states:\n",
        "        pi[s] = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
        "    return pi\n",
        "\n",
        "\n",
        "def expected_utility(a, s, U, mdp):\n",
        "    \"\"\"\n",
        "    Return the expected utility of taking action a in state s.\n",
        "\n",
        "    Rewards depend on (state, action), not just state. So we use mdp.R(s,a) to get the immediare reward and add this to the discounted future reward.\n",
        "    \"\"\"\n",
        "\n",
        "    immediate = mdp.R(s, a)\n",
        "    future = sum(p * U[s1] for (p, s1) in mdp.T(s, a))\n",
        "\n",
        "    return immediate + mdp.gamma * future\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# Smart Building MDP\n",
        "\n",
        "class SmartBuildingMDP(MDP):\n",
        "\n",
        "    def __init__(self, init, actlist, terminals, transitions, reward_sa, gamma=0.9):\n",
        "        states = self.get_states_from_transitions(transitions)\n",
        "        super().__init__(init, actlist, terminals,\n",
        "                         transitions=transitions,\n",
        "                         reward={s: 0 for s in states},\n",
        "                         states=states,\n",
        "                         gamma=gamma)\n",
        "        self.reward_sa = reward_sa\n",
        "\n",
        "    def R(self, state, action):\n",
        "        if action is None:\n",
        "            return 0.0\n",
        "        return float(self.reward_sa[(state, action)])\n",
        "\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# Define Smart Building transitions & rewards\n",
        "\n",
        "transitions = {\n",
        "    \"L\": {\"E\": [(1.0, \"L\")],\n",
        "          \"B\": [(0.5, \"M\"), (0.5, \"L\")]},\n",
        "\n",
        "    \"M\": {\"E\": [(0.7, \"M\"), (0.3, \"L\")],\n",
        "          \"B\": [(0.6, \"H\"), (0.4, \"M\")]},\n",
        "\n",
        "    \"H\": {\"E\": [(0.8, \"M\"), (0.2, \"H\")],\n",
        "          \"B\": [(1.0, \"H\")]}\n",
        "}\n",
        "\n",
        "reward_sa = {\n",
        "    (\"L\",\"E\"): 1,\n",
        "    (\"L\",\"B\"): 3,\n",
        "    (\"M\",\"E\"): 2,\n",
        "    (\"M\",\"B\"): 4,\n",
        "    (\"H\",\"E\"): 1,\n",
        "    (\"H\",\"B\"): -8\n",
        "}\n",
        "\n",
        "mdp = SmartBuildingMDP(\n",
        "    init=\"L\",\n",
        "    actlist=[\"E\",\"B\"],\n",
        "    terminals=[],\n",
        "    transitions=transitions,\n",
        "    reward_sa=reward_sa,\n",
        "    gamma=0.9\n",
        ")\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# Run value iteration and print results\n",
        "\n",
        "print(\"Smart Building MDP\")\n",
        "print(\"Policy for L, M, H Energy States\")\n",
        "print(\"-------------------------\")\n",
        "U_star = value_iteration(mdp)\n",
        "\n",
        "print(\"\\nV*: \" + \", \".join(f\"{s}={U_star[s]:.6f}\" for s in [\"L\",\"M\",\"H\"]))\n",
        "\n",
        "policy = best_policy(mdp, U_star)\n",
        "print(\"pi*: \" + \", \".join(f\"{s}->{policy[s]}\" for s in [\"L\",\"M\",\"H\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzoeNTAhv4eJ",
        "outputId": "bda6133c-480e-413f-f613-bd1ecd1e10b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smart Building MDP\n",
            "Policy for L, M, H Energy States\n",
            "-------------------------\n",
            "V0: 0.0, 0.0, 0.0\n",
            "-------------------------\n",
            "V1: 3.0, 4.0, 1.0\n",
            "-------------------------\n",
            "V2: 6.15, 5.98, 4.06\n",
            "-------------------------\n",
            "V3: 8.4585, 8.3452, 6.0364\n",
            "-------------------------\n",
            "\n",
            "V*: L=28.434840, M=28.087246, H=25.881364\n",
            "pi*: L->B, M->B, H->E\n"
          ]
        }
      ]
    }
  ]
}